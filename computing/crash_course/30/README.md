# The World Wide Web
[Video Link](https://youtu.be/guvsH5OFizE)

The [world wide web](../glossary/README.md#world-wide-web) is not the same as the [internet](../glossary/README.md#internet) although people often use the terms interchangeably in everyday language. The world wide web runs on top of the internet. The internet is the underlying plumbing that conveys the data for a myriad of different applications and the world wide web is the biggest of them all: a huge distributed application running on millions of [servers](../glossary/README.md#web-server) worldwide, accessed using a type of program called a [web browser](../glossary/README.md#web-browser).

The fundamental building block of the world wide web is a single [page](../glossary/README.md#web-page). This is a document, containing content, which can include links to other pages. These links are called [hyperlinks](../glossary/README.md#hyperlink): text or images that can be clicked to load another page. These hyperlinks form a web of interconnected information which is where the world wide web gets its name.

Before hyperlinks were implemented, in order to access a specific piece of information on a computer, it had to be accessed directly from the file system or accessed through a search. With hyperlinks a user can easily flow frokm one related topic to another. The value of hyperlinked information was conceptualized by [Vannevar Bush](https://en.wikipedia.org/wiki/Vannevar_Bush) in 1945. He had published an [article](https://en.wikipedia.org/wiki/As_We_May_Think) describing a hypothetical machined called a [Memex](https://en.wikipedia.org/wiki/Memex) (as previously discussed). Bush described the concept as ["_associative indexing_"](https://www.azquotes.com/quote/653953):

> Associative indexing, the basic idea of which is a provision whereby any item may be caused at will to select immediately and automatically another. This is the essential feature of the Memex. The process of tying two items together is the important thing. Thereafter, at any time, when one of those items is in view, the other [item] can be installed recalled merely by tapping a button.

Text containing hyperlinks is called [hypertext](../glossary/README.md#hypertext). Web pages, the most common type of hypertext document today, are retrieved and rendered by web browsers. In order for pages to link to one another each hypertext page needs a unique address, which on the web is specified by a [Uniform Resource Locator](../glossary/README.md#url) (_URL_).

When a web browser requests a web site, the first thing that happens is a [DNS](../glossary/README.md#domain-name-system) lookup: this takes a domain name as input and replies back with the corresponding computer's [IP address](../glossary/README.md#ip-address). Next, using that address, the web browser opens a [TCP](../glossary/README.md#transmission-control-protocol) connection to a computer that is running a piece of software called a _web server_. The standard [port](../glossary/README.md#port) number for web servers is port 80. After this TCP connection is established, the web browser requests the page at the URL. To do this, the browser uses the [Hypertext Transfer Protocol](../glossary/README.md#hypertext-transfer-protocol), or _HTTP_. The very first documented version of the HTTP specification, HTTP 0.9, created in 1991, only had one command: `GET`. The web browser issues this `GET` command to the web server, as a raw [ASCII](../glossary/README.md#ascii) text, which then replies back with the web page hypertext requested. This hypertext is then interpreted by the web browser and rendered to the screen. If the user clicks on a link to another page, the browser simply issues another `GET` request to the appropriate web server.

In later versions HTTP added status codes, which prefixed any hypertext that was sent following a `GET` request. For example: status code 200 means OK. Status codes in the 400s are for client errors, such as a request for a page that doesn't exist (404 error). Web page hypertext is stored and sent as plain text, for example encoded in ASCII or UTF-16. Because plain text files don't have a mechanism by which to specify what is a link and what isn't, it was necessary to develop a way to "mark up" a text file with hypertext elements. For this reason the [Hypertext Markup Language](../glossary/README.md#html) was developed.

The very first version of HTML, version 0.a (created in 1990), provided 18 HTML commands to mark up pages. One example of an HTML tag is `<h1></h1>` which specifies a page heading. A second-level heading is specified with the H2 tag `<h2></h2>` The anchor tag (`<a></a>`) is used to specify content to be hyperlinked to another resource. The tag for an ordered list is `<ol></ol>` while list items are `<li></li>`.

Today's web browsers are more sophisticated: the newest version of HTML, version 5, has over a hundred different tags - for things like images, tables, forms, and buttons. Other technologies, like [Cascading Style Sheets](../glossary/README.md#css) (_CSS_) and [JavaScript](../glossary/README.md#javascript) can be embedded into HTML pages to provide styling and functionality.

Browsers will request pages and media, and then render the content that's returned. The first web browser, and web server, was written by [Tim Berners-Lee](https://en.wikipedia.org/wiki/Tim_Berners-Lee) over the course of two months in 1990. At the time he was working at [CERN](https://en.wikipedia.org/wiki/CERN) in Switzerland. To achieve this task Tim simultaneously created several of the fundamental web standards already discussed: URLs, HTML, and HTTP. After initially circulating his software amongst colleagues at CERN, it was released to the public in 1991. Importantly the web was an open standard, making it possible for anyone to develop new web servers and browsers.

Taking advantage of this, a team at the [University of Illinois at Urbana-Champaign](https://en.wikipedia.org/wiki/University_of_Illinois_at_Urbana%E2%80%93Champaign) to create the [Mosaic web browser](https://en.wikipedia.org/wiki/Mosaic_(web_browser)) in 1993. It was the first browser that allowed graphics to be embedded alongside text. Previous browsers displayed graphics in separate windows. It also introduced new features like bookmarks, and had a friendly GUI interface.

![Screenshot of NCSA Mosaic 2.7 for Unix](./NCSA_Mosaic_Browser.png)
<br /><br />

By the end of the 1990s there were many web browsers in use, like [Netscape Navigator](https://en.wikipedia.org/wiki/Netscape_Navigator), [Internet Explorer](https://en.wikipedia.org/wiki/Internet_Explorer), [Opera](https://en.wikipedia.org/wiki/Opera_(web_browser)), [OmniWeb](https://en.wikipedia.org/wiki/OmniWeb), and [Mozilla](https://en.wikipedia.org/wiki/Firefox). Many web servers were also developed, like [Apache](https://en.wikipedia.org/wiki/Apache_HTTP_Server) and [Microsoft's Internet Information Services](https://en.wikipedia.org/wiki/Internet_Information_Services). New websites popped up daily and web mainstays like [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) and [eBay](https://en.wikipedia.org/wiki/EBay) were founded in the mid-1990s. The web was flourishing and people increasingly needed ways to find things.

At first people maintained web pages which served as directories hyperlinking to other websites. Most famous among these was [Jerry and David's Guide to the World Wide Web](https://en.wikipedia.org/wiki/History_of_Yahoo!), renamed [Yahoo!](https://en.wikipedia.org/wiki/Yahoo!) in 1994. As the web grew, these human-edited directories started to get unweildy, and so [search engines](../glossary/README.md#search-engine) were developed. The earliest web search engine that operated like the ones in use today was [JumpStation](https://en.wikipedia.org/wiki/JumpStation), created by Jonathan Fletcher in 1993 at the [University of Stirling](https://en.wikipedia.org/wiki/University_of_Stirling). This search engine consisted of three pieces of software that worked together:
* A [web crawler](../glossary/README.md#web-crawler): software that follows all the links it can find on the web. Anytime it follows a link to a page that has new links, it would add the new links to its list.
* An ever-growing [index](../glossary/README.md#web-indexing), recording what text terms appear on what pages the crawler had visited.
* A [search algorithm](https://en.wikipedia.org/wiki/Search_algorithm) that consults the index.

Early search engines used very simple metrics to rank order their search results, most often the number of times a search time appears on a page. This worked well until websites started to game the system by writing specific terms hundreds of times on their pages to steer traffic in their direction. [Google's](https://en.wikipedia.org/wiki/Google) rise to fame was in large part due to a clever algorithm that sidestepped this issue: instead of trusting the content on a web page, they looked at how other websites linked to a given page.  The number of [backlinks](../glossary/README.md#backlink), especially from reputable sites, was often a good indicator of quality. This strategy began as a research project called [BackRub](https://en.wikipedia.org/wiki/History_of_Google) at [Stanford University](https://en.wikipedia.org/wiki/Stanford_University) in 1996, before being spun out two years later into the Google we know today.

[Network neutrality](../glossary/README.md#net-neutrality) is the principle that all packets on the internet should be treated equally. It doesn't matter if the packets are email or streaming video, they should all be passed along the World Wide Web at the same speed and priority. Many companies would pefer that their data arrive to a computer preferentially. Without net neutrality, an internet service provider could choose which packets to give priority to, and which sources to [throttle](../glossary/README.md#bandwidth-throttling). Net neutrality advocates argue that giving inernet service providers the ability to set up tolls on the internet (to provide premium package delivery) plants the seeds for an exploitative business model. ISPs could be gatekeepers to content, with strong incentives to note play fair with competitors. Also, big companies could pay to get special treament - smaller companies would be at disadvantage, stifling innovation.

On the other hand there are good technical reasons why a user may want different types of data to flow at different speeds: a video call requires high priority, but it is not a big deal if an email comes in a few seconds late. Net-neutrality opponents also argue that market forces and competition would discourage bad behavior because customers would leave ISPs that are throttling sites that they frequent.

| [Previous: The Internet](../29/README.md) | [Table of Contents](../README.md#table-of-contents) | [Next: Cybersecurity](../31/README.md) |
